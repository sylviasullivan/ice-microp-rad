{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from xhistogram.xarray import histogram as xhist\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys, os, warnings\n",
    "#sys.path.append(os.path.abspath(\"/work/bb1018/b380873/tropic_vis/utilities/\"))\n",
    "sys.path.append(os.path.abspath(\"/xdisk/sylvia/tropic_vis/utilities/\"))\n",
    "from plotting_utilities import *\n",
    "from calc_water import calc_water"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "def allDone():\n",
    "    display(Audio('/xdisk/sylvia/traj_output/Hallelujah-sound-effect.mp3', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings( \"ignore\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cdo commands to adjust the ICON nc variable names and contents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "cdo chname,t,T,qni,Ni ICON_file.nc ICON_file2.nc\n",
    "cdo merge -expr,'psati=exp(9.550426-5723.265/T+3.53068*log(T)-0.00728332*T)' ICON_file2.nc ICON_file2.nc ICON_file3.nc\n",
    "cdo merge -expr,'RHI=qv*100/0.622/psati*(p-psati)' ICON_file3.nc ICON_fil3.nc ICON_file4.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all ICON trajectories into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the ICON-1M trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_ICON_0V1M0A0R/'\n",
    "new_file = xr.open_dataset( basedir + 'traj_tst00001350_p001_trim_extract_dt_filter.nc' )\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'traj_tst00001350_p' + traj_prefix(i) + str(i) + '_trim_extract_dt_filter.nc' ) # ICON\n",
    "    new_file = xr.concat( (new_file, file2), dim='id' )\n",
    "    \n",
    "display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'traj_tst00001350_trim_extract_dt_filter.nc' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the ICON-2M trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_ICON_0V2M0A0R/'\n",
    "new_file = xr.open_dataset( basedir + 'traj_tst00000450_p001_trim_extract_dt_filter.nc' )\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'traj_tst00000450_p' + traj_prefix(i) + str(i) + '_trim_extract_dt_filter.nc' ) # ICON\n",
    "    new_file = xr.concat( (new_file, file2), dim='id' )\n",
    "    \n",
    "display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'traj_tst00000450_trim_extract_dt_filter.nc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cdo commands to adjust the CLaMS nc variables names and contents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "for i in $(ls cirrus_tst*); do cdo settaxis,2017-08-06,09:00:00,24sec $i ${i%'.nc'}'_dt.nc'; done\n",
    "for i in $(ls cirrus_tst*dt.nc); do cdo merge -expr,'qi=IWC_het+IWC_hom+IWC_pre;Ni=ICN_pre+ICN_het+ICN_hom' -chname,RHO,rho $i $i ${i%'.nc'}'_iwc.nc'; done\n",
    "module load nco/4.7.5-gcc64\n",
    "for i in $(ls cirrus_tst*iwc.nc); do ncrename -d NPARTS,id $i ${i%'.nc'}'2.nc'; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all CLaMS trajectories into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the CLAMS trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_CLAMS-Tf_0V2M0A0R/'\n",
    "new_file = xr.open_dataset( basedir + 'cirrus_tst00000450_p001_trim_extract_clams_dt_iwc_filter.nc' )\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'cirrus_tst00000450_p' + traj_prefix(i) + str(i) + '_trim_extract_clams_dt_iwc_filter.nc' )\n",
    "    new_file = xr.concat( (new_file, file2), dim='NPARTS' )\n",
    "    \n",
    "#display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'cirrus_tst00000450_trim_extract_clams_dt_iwc_filter.nc' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the CLAMS trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_CLAMS-Tf_0V1M0A0R/'\n",
    "new_file = xr.open_dataset( basedir + 'cirrus_tst00001350_p001_trim_extract_clams_dt_iwc_filter.nc' )\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'cirrus_tst00001350_p' + traj_prefix(i) + str(i) + '_trim_extract_clams_dt_iwc_filter.nc' )\n",
    "    new_file = xr.concat( (new_file, file2), dim='NPARTS' )\n",
    "    \n",
    "#display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'cirrus_tst00001350_trim_extract_clams_dt_iwc_filter.nc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all trajectories for 5 simulations from 5 files\n",
    "Initiate the binning for different variables here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These thresholds are used throughout. Values below are considered negligible.\n",
    "qi_threshold = 10**(-8)\n",
    "Ni_threshold = 10**(-8)\n",
    "RHi_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#basedir = '/work/bb1018/b380873/traj_output/'\n",
    "basedir = '/xdisk/sylvia/traj_output/'\n",
    "fichiers = [ 'CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc', 'ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc',\n",
    "             'CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc', 'ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc',\n",
    "             'CLAMS_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc', 'CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc' ]\n",
    "\n",
    "# Logarithmic IWC bins in ppmv, as well as their centerpoints\n",
    "qi_bins = np.logspace( -5, 3, 50 )\n",
    "qi_bins_c = ( qi_bins[1:] + qi_bins[:-1] )/2.\n",
    "\n",
    "# Logarithmic Ni bins in L-1, as well as their centerpoints\n",
    "Ni_bins = np.logspace( -4, 4, 50 )\n",
    "Ni_bins_c = ( Ni_bins[1:] + Ni_bins[:-1] )/2.\n",
    "\n",
    "# Linear T bins in K\n",
    "T_bins = np.linspace( 190, 240, 50 )\n",
    "T_bins_c = ( T_bins[1:] + T_bins[:-1] )/2.\n",
    "\n",
    "# Linear RHi bins in %\n",
    "RHi_bins = np.linspace( 60, 120, 50 )\n",
    "RHi_bins_c = ( RHi_bins[1:] + RHi_bins[:-1] )/2.\n",
    "\n",
    "# Flight 7 track times\n",
    "time0 = datetime(2017, 8, 8, 6, 20)\n",
    "timef = datetime(2017, 8, 8, 6, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a generically structured dataset to store histogram values and copy it 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially all the datasets contain the same variables so we can create a generic dataset...\n",
    "ds_generic = xr.Dataset( \n",
    "    data_vars=dict(  qih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     Th=(['T_bin'], np.zeros(T_bins_c.shape)),\n",
    "                     qih_outflow=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih_outflow=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qih_insitu=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih_insitu=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qih_flight=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih_flight=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     RHih=(['RHi_bin'], np.zeros(RHi_bins_c.shape)) ),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c),\n",
    "                  T_bin=(['T_bin'], T_bins_c),\n",
    "                  RHi_bin=(['RHi_bin'], RHi_bins_c) ) )\n",
    "\n",
    "# ... and then copy it 6 times.\n",
    "datasets =  [ ds_generic, ds_generic.copy(), ds_generic.copy(), ds_generic.copy(), ds_generic.copy(), ds_generic.copy() ]\n",
    "del ds_generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different 'flavors' of qi and Ni histograms calculated and mean / median qi / Ni printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RHi_1M0T1S = xr.open_dataset( basedir + fichiers[0] )['RHI']\n",
    "RHi_2M1T1S = xr.open_dataset( basedir + fichiers[2] )['RHI']\n",
    "RHi_2M0T1S = xr.open_dataset( basedir + fichiers[4] )['RHI']\n",
    "RHi_2M1T0S = xr.open_dataset( basedir + fichiers[5] )['RHI']\n",
    "RHi_fi = [ RHi_1M0T1S, RHi_1M0T1S, RHi_2M1T1S, RHi_2M1T1S, RHi_2M0T1S, RHi_2M1T0S ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "0\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "1\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "2\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "3\n",
      "CLAMS_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc\n",
      "4\n",
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "5\n",
      "CPU times: user 2min 29s, sys: 1min 2s, total: 3min 32s\n",
      "Wall time: 6min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the qih_* values in the Datasets above\n",
    "means = np.zeros( (6,3) )\n",
    "medians = np.zeros( (6,3) )\n",
    "\n",
    "for f, d, r, j in zip(fichiers, datasets, RHi_fi, np.arange(6)):\n",
    "#for f, d in zip(fichiers, datasets):\n",
    "    print(f)\n",
    "    #k = 700 #2000\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Take only the first 2000 trajectories to see if bimodality is caused by different trajs k=2000 and [:k]\n",
    "    # Take only the first 700 timepoints to see if bimodality is caused by different times k=700 and [:,:k]\n",
    "    qi = fi['qi']\n",
    "    T = fi['T']\n",
    "    if (j != 1) & (j != 3):\n",
    "        P = fi['PE']\n",
    "    else:\n",
    "        P = fi['p']\n",
    "    time = fi['time']\n",
    "    \n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi_ppmv = calc_water( T, P, qi )\n",
    "    qi_ppmv.name = 'qi_ppmv'\n",
    "    # Ensure that qi [kg kg-1] is not mistakenly used below\n",
    "    del qi\n",
    "\n",
    "    # Filter for non-negligible qi (or now CLaMS RHi) values; side-note: large peak in ICON qi at 10**(-10)\n",
    "    #qi = qi.where( qi > qi_threshold )\n",
    "    qi_ppmv = qi_ppmv.where( (r > RHi_threshold ) & (qi_ppmv > qi_threshold) )\n",
    "    qi_outflow = qi_ppmv.where( (T >= 210) & (T <= 237) )\n",
    "    qi_insitu = qi_ppmv.where( (T < 210) )\n",
    "    #qi_flight = qi.sel( time=slice(time0, timef) )\n",
    "    \n",
    "    #means[j,0] = np.nanmean(qi)\n",
    "    #medians[j,0] = np.nanmedian(qi)\n",
    "    #means[j,1] = np.nanmean(qi_outflow)\n",
    "    #medians[j,1] = np.nanmedian(qi_outflow)\n",
    "    #means[j,2] = np.nanmean(qi_insitu)\n",
    "    #medians[j,2] = np.nanmedian(qi_insitu)\n",
    "\n",
    "    #print(np.nanmean(qi_flight),np.nanmedian(qi_flight))\n",
    "    #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_outflow, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_insitu, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih_insitu'] = h\n",
    "    \n",
    "    #wgts = 1*xr.apply_ufunc( np.isfinite, qi_flight )\n",
    "    #wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    #h = xhist( qi_flight, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    #d['qih_flight'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, T )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( T, dim=['time','id'], block_size=100, weights=wgts, bins=[T_bins] )\n",
    "    d['Th'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "ICON 1-mom.. geht es weiter!\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "CLAMS_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc\n",
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "CPU times: user 1min 25s, sys: 38.8 s, total: 2min 3s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the Nih_* values in the Datasets above.\n",
    "means = np.zeros( (6,3) )\n",
    "medians = np.zeros( (6,3) )\n",
    "\n",
    "for f, d, r, j in zip(fichiers, datasets, RHi_fi, np.arange(6)):\n",
    "#for f, d, j in zip(fichiers, datasets, np.arange(4)):\n",
    "    print(f)\n",
    "    #k = 700\n",
    "    if j == 1:\n",
    "        print('ICON 1-mom.. geht es weiter!')\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        continue\n",
    "    else:\n",
    "        fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "        # Convert kg-1 to L-1\n",
    "        rho = fi['rho'] #[:k]\n",
    "        Ni = fi['Ni'] * rho  / 1000.\n",
    "        Ni.name = fi['Ni'].name\n",
    "        T = fi['T']\n",
    "        time = fi['time']\n",
    "        #r = r\n",
    "    \n",
    "        # Filter for non-negligible values\n",
    "        #Ni = Ni.where( (Ni > Ni_threshold) )\n",
    "        Ni = Ni.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "        Ni_outflow = Ni.where( (T >= 210) & (T <= 237) )\n",
    "        Ni_insitu = Ni.where( (T < 210) )\n",
    "        #Ni_flight = Ni.sel( time=slice(time0, timef) )\n",
    "        \n",
    "        #means[j,0] = np.nanmean(Ni)\n",
    "        #medians[j,0] = np.nanmedian(Ni)\n",
    "        #means[j,1] = np.nanmean(Ni_outflow)\n",
    "        #medians[j,1] = np.nanmedian(Ni_outflow)\n",
    "        #means[j,2] = np.nanmean(Ni_insitu)\n",
    "        #medians[j,2] = np.nanmedian(Ni_insitu)\n",
    "\n",
    "        #print(np.nanmean(Ni_flight),np.nanmedian(Ni_flight))\n",
    "        #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_outflow, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_insitu, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih_insitu'] = h\n",
    "    \n",
    "    #wgts = 1*xr.apply_ufunc( np.isfinite, Ni_flight )\n",
    "    #wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    #h = xhist( Ni_flight, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    #d['Nih_flight'] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE and RHI histograms calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "CLAMS_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc\n",
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "ICON guy.. geht es weiter!\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "ICON guy.. geht es weiter!\n",
      "CLAMS_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc\n",
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "CPU times: user 1min 1s, sys: 31.5 s, total: 1min 33s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#basedir = '/work/bb1018/b380873/traj_output/'\n",
    "basedir = '/xdisk/sylvia/traj_output/'\n",
    "# Save the RHIh_ values for both datasets and the TEh_* values for the CLaMS Datasets only\n",
    "for f, d in zip(fichiers, datasets):\n",
    "    print(f)\n",
    "    #k = 700\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    qi = fi['qi'] * 10**6  #[:k]\n",
    "    RHi = fi['RHI'].where( qi > qi_threshold )\n",
    "    #print(np.nanmean(RHi),np.nanmedian(RHi))\n",
    "    #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, RHi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( RHi, dim=['time','id'], block_size=100, weights=wgts, bins=[RHi_bins] )\n",
    "    d['RHih'] = h\n",
    "\n",
    "for f, d, j in zip(fichiers, datasets, np.arange(6)):\n",
    "    print(f)\n",
    "    #k = 700\n",
    "    if j == 1 or j == 3:\n",
    "        print('ICON guy.. geht es weiter!')\n",
    "        #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        continue\n",
    "    else:\n",
    "        fi = xr.open_dataset( basedir + f )\n",
    "        qi = fi['qi'] * 10**6\n",
    "        TE = fi['TE'].where( qi > qi_threshold )\n",
    "        #print(np.nanmean(TE),np.nanmedian(TE))\n",
    "        #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        \n",
    "        # Weight by the total number of non-nan values\n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, TE )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( TE, dim=['time','id'], block_size=100, weights=wgts, bins=[T_bins] )\n",
    "        d['TEh'] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qi-Nih-T-RHi 1D histograms saved to nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, j in zip(datasets, np.arange(6)):\n",
    "    d['qih'].attrs['description'] = \"1D hist values of ice mass mixing ratio across qi_bin\" \n",
    "    d['qih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Nih'].attrs['description'] = \"1D hist values of ice crystal number concentration across Ni_bin\" \n",
    "    d['Nih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Th'].attrs['description'] = \"1D hist values of temperature across T_bin\" \n",
    "    d['Th'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qih_outflow'].attrs['description'] = \"1D hist values of ice mass mixing ratio for T >= 210 K across qi_bin\" \n",
    "    d['qih_outflow'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Nih_outflow'].attrs['description'] = \"1D hist values of ice crystal number concentration for T >= 210 K across Ni_bin\" \n",
    "    d['Nih_outflow'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qih_insitu'].attrs['description'] = \"1D hist values of ice mass mixing ratio for T < 210 K across qi_bin\" \n",
    "    d['qih_insitu'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Nih_insitu'].attrs['description'] = \"1D hist values of ice crystal number concentration for T < 210 K across Ni_bin\" \n",
    "    d['Nih_insitu'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qih_flight'].attrs['description'] = \"1D hist values of ice mass mixing ratio for StratoClim Flight 7 times across qi_bin\" \n",
    "    d['qih_flight'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Nih_flight'].attrs['description'] = \"1D hist values of ice crystal number concentration for StratoClim Flight 7 times across Ni_bin\" \n",
    "    d['Nih_flight'].attrs['units'] = \"Probability [%]\"\n",
    "    d['RHih'].attrs['description'] = \"1D hist values of relative humidity wrt ice across RHI_bin\" \n",
    "    d['RHih'].attrs['units'] = \"Probability [%]\"\n",
    "    if j == 0 or j == 2 or j == 4 or j == 5:\n",
    "        d['TEh'].attrs['description'] = \"1D hist values of external temperature (includes latent heating) across TE_bin (only for CLaMS)\"\n",
    "        d['TEh'].attrs['units']=\"Probability [%]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the qi and Ni histogram datasets in nc files\n",
    "#writedir = '/work/bb1018/b380873/traj_output/traj_pp/'\n",
    "writedir = '/xdisk/sylvia/traj_output/traj_pp/'\n",
    "names = [ 'qih-Nih-CLAMS-Tf_0V1M0A0R.nc', 'qih-Nih-ICON_0V1M0A0R.nc', \n",
    "          'qih-Nih-CLAMS-Tf_0V2M0A0R.nc', 'qih-Nih-ICON_0V2M0A0R.nc',\n",
    "          'qih-Nih-CLAMS_0V2M0A0R.nc', 'qih-Nih-CLAMS-Tf_0V2M0A0R_noSHflux.nc' ]\n",
    "for n, d in zip(names, datasets):\n",
    "    d.to_netcdf( writedir + n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ice microphysical process histograms\n",
    "These differ between CLaMS and ICON so we have to initially create two different structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For CLaMS, we will look at the homogeneous, heterogeneous, and preexisiting qi and Ni\n",
    "ds_clams = xr.Dataset( \n",
    "    data_vars=dict(  qhomih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nhomih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qhetih=(['qi_bin'], np.zeros(T_bins_c.shape)),\n",
    "                     Nhetih=(['Ni_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     qpreih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Npreih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)) ),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c) ) )\n",
    "\n",
    "# ... and then copy it 4 times.\n",
    "datasets_clams = [ ds_clams, ds_clams.copy() ]#, ds_clams.copy(), ds_clams.copy() ]\n",
    "del ds_clams"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# For ICON, we will look at qi and Ni sedimentation fluxes in and out\n",
    "ds_icon = xr.Dataset( \n",
    "    data_vars=dict(  qsedih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nsedih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qsedoh=(['qi_bin'], np.zeros(T_bins_c.shape)),\n",
    "                     Nsedoh=(['Ni_bin'], np.zeros(qi_bins_c.shape)) ),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c) ) )\n",
    "\n",
    "# We only need one dataset because we did not save sedimentation tendencies along the 1-mom trajectories.\n",
    "datasets_icon = [ ds_icon ]\n",
    "del ds_icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n"
     ]
    }
   ],
   "source": [
    "clams_fichiers = [ fichiers[0], fichiers[2] ]#, fichiers[4], fichiers[5] ]\n",
    "for f, d in zip(clams_fichiers, datasets_clams):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert to ppmv / L-1 and filter for non-negligible values\n",
    "    qi_hom = fi['IWC_hom']\n",
    "    P = fi['PE']\n",
    "    T = fi['T']\n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi_hom_ppmv = calc_water( T, P, qi_hom )\n",
    "    qi_hom_ppmv.name = 'qi_hom_ppmv'\n",
    "    # Ensure that qi_hom [kg kg-1] is not mistakenly used below\n",
    "    del qi_hom\n",
    "    qi_hom_ppmv = qi_hom_ppmv.where( qi_hom_ppmv > qi_threshold )\n",
    "    \n",
    "    qi_het = fi['IWC_het']\n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi_het_ppmv = calc_water( T, P, qi_het )\n",
    "    qi_het_ppmv.name = 'qi_het_ppmv'\n",
    "    # Ensure that qi_het [kg kg-1] is not mistakenly used below\n",
    "    del qi_het\n",
    "    qi_het_ppmv = qi_het_ppmv.where( qi_het_ppmv > qi_threshold )\n",
    "    \n",
    "    qi_pre = fi['IWC_pre']\n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi_pre_ppmv = calc_water( T, P, qi_pre )\n",
    "    qi_pre_ppmv.name = 'qi_pre_ppmv'\n",
    "    # Ensure that qi_pre [kg kg-1] is not mistakenly used below\n",
    "    del qi_pre\n",
    "    qi_pre_ppmv = qi_pre_ppmv.where( qi_pre_ppmv > qi_threshold )\n",
    "    \n",
    "    rho = fi['rho']\n",
    "    Ni_hom = fi['ICN_hom'] * rho / 1000\n",
    "    Ni_hom.name = fi['ICN_hom'].name\n",
    "    Ni_hom = Ni_hom.where( Ni_hom > Ni_threshold )\n",
    "    \n",
    "    Ni_het = fi['ICN_het'] * rho / 1000\n",
    "    Ni_het.name = fi['ICN_het'].name\n",
    "    Ni_het = Ni_hom.where( Ni_het > Ni_threshold )\n",
    "    \n",
    "    Ni_pre = fi['ICN_pre'] * rho / 1000\n",
    "    Ni_pre.name = fi['ICN_pre'].name\n",
    "    Ni_pre = Ni_pre.where( Ni_pre > Ni_threshold )\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_hom_ppmv )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_hom_ppmv, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qhomih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_het_ppmv )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_het_ppmv, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qhetih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_pre_ppmv )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_pre_ppmv, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qpreih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_hom )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_hom, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nhomih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_het )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_het, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nhetih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_pre )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_pre, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Npreih'] = h"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "%%time\n",
    "for f, d in zip([fichiers[3]], datasets_icon):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert to ppmv and filter for non-negligible values\n",
    "    # We want the magnitude of these sedimentation values so multiply by -1.\n",
    "    qsedi = -1*fi['qsedi'] * 10**6\n",
    "    qsedi = qsedi.where( qsedi > qi_threshold )\n",
    "    qsedo = -1*fi['qsedo'] * 10**6\n",
    "    qsedo = qsedo.where( qsedo > qi_threshold )\n",
    "    \n",
    "    # As above but converting to L-1\n",
    "    rho = fi['rho']\n",
    "    qnsedi = fi['qnsedi'] * rho / 1000.\n",
    "    qnsedi.name = fi['qnsedi'].name\n",
    "    qnsedi = -1*qnsedi.where( -1*qnsedi > Ni_threshold )  \n",
    "    qnsedo = fi['qnsedo'] * rho / 1000.\n",
    "    qnsedo.name = fi['qnsedo'].name\n",
    "    qnsedo = -1*qnsedo.where( -1*qnsedo > Ni_threshold )\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qsedi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qsedi, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qsedih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qsedo )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qsedo, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qsedoh'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qnsedi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qnsedi, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nsedih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qnsedo )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qnsedo, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nsedoh'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for d in datasets_clams:\n",
    "    d['qhomih'].attrs['description'] = \"1D hist values of ice mass mixing ratio from homogeneous nucleation across qi_bin\" \n",
    "    d['qhomih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Nhomih'].attrs['description'] = \"1D hist values of ice crystal number concentration from homogeneous nucleation across Ni_bin\" \n",
    "    d['Nhomih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qhetih'].attrs['description'] = \"1D hist values of ice mass mixing ratio from heterogeneous nucleation across qi_bin\" \n",
    "    d['qhetih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Nhetih'].attrs['description'] = \"1D hist values of ice crystal number concentration from heterogeneous nucleation across Ni_bin\" \n",
    "    d['Nhetih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qpreih'].attrs['description'] = \"1D hist values of ice mass mixing ratio from preexisting ice across qi_bin\" \n",
    "    d['qpreih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['Npreih'].attrs['description'] = \"1D hist values of ice crystal number concentration from preexisting ice across Ni_bin\" \n",
    "    d['Npreih'].attrs['units'] = \"Probability [%]\"\n",
    "    \n",
    "#for d in datasets_icon:\n",
    "#    d['qsedih'].attrs['description'] = \"1D hist values of ice mass mixing ratio sedimenting in across qi_bin\" \n",
    "#    d['qsedih'].attrs['units'] = \"Probability [%]\"\n",
    "#    d['Nsedih'].attrs['description'] = \"1D hist values of ice crystal number concentration sedimenting in across Ni_bin\" \n",
    "#    d['Nsedih'].attrs['units'] = \"Probability [%]\"\n",
    "#    d['qsedoh'].attrs['description'] = \"1D hist values of ice mass mixing ratio sedimenting out across qi_bin\" \n",
    "#    d['qsedoh'].attrs['units'] = \"Probability [%]\"\n",
    "#    d['Nsedoh'].attrs['description'] = \"1D hist values of ice crystal number concentration sedimenting out across Ni_bin\" \n",
    "#    d['Nsedoh'].attrs['units'] = \"Probability [%]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "writedir = '/work/bb1018/b380873/traj_output/traj_pp/'\n",
    "writedir = '/xdisk/sylvia/traj_output/traj_pp/'\n",
    "names = [ 'het-hom-pre-CLAMS-Tf_0V1M0A0R.nc', 'het-hom-pre-CLAMS-Tf_0V2M0A0R.nc' ]#, 'sed-ICON_0V2M0A0R.nc',\n",
    "         #'het-hom-pre-CLAMS_0V2M0A0R.nc', 'het-hom-pre-CLAMS-Tf_0V2M0A0R_noSHflux.nc' ]\n",
    "datasets = [ datasets_clams[0], datasets_clams[1] ]#, datasets_icon[0], datasets_clams[2], datasets_clams[3] ]\n",
    "\n",
    "for n, d in zip(names, datasets):\n",
    "    d.to_netcdf( writedir + n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional histograms - qi, Ni versus T, RHi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of qi and Ni bins relative to the 1D histograms\n",
    "\n",
    "# Logarithmic IWC bins in ppmv, as well as their centerpoints\n",
    "qi_bins = np.logspace( -5, 3.5, 25 )\n",
    "qi_bins_c = ( qi_bins[1:] + qi_bins[:-1] )/2.\n",
    "\n",
    "# Logarithmic Ni bins in cm-3, as well as their centerpoints\n",
    "Ni_bins = np.logspace( -4, 5.5, 25 )\n",
    "Ni_bins_c = ( Ni_bins[1:] + Ni_bins[:-1] )/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StratoClim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sc = xr.open_dataset( '/xdisk/sylvia/tropic_vis/obs/stratoclim2017.geophysika.0808_1.filtered_per_sec.nc')\n",
    "sc_qi = sc['BEST:IWC']\n",
    "sc_temp = sc['BEST:TEMP']\n",
    "\n",
    "sc_qi = sc_qi.where( (sc_qi > qi_threshold) )\n",
    "sc_temp = sc_temp.where( (sc_qi > qi_threshold) )\n",
    "\n",
    "wgts = 1*xr.apply_ufunc( np.isfinite, sc_qi )\n",
    "wgts = wgts / wgts.sum( dim=['time'] ) * 100.\n",
    "\n",
    "h = xhist( sc_qi, sc_temp, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "qs = qi_bins_c.shape[0]\n",
    "ts = T_bins_c.shape[0]\n",
    "ds_generic = xr.Dataset( data_vars=dict( qiTh_flight=(['qi_bin','T_bin'], np.zeros((qs, ts)) ) ) )\n",
    "ds_generic['qiTh_flight'] = h\n",
    "ds_generic.to_netcdf( '/xdisk/sylvia/tropic_vis/obs/stratoclim2017.geophysika.0808_1_hist.nc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using simulation trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially all the datasets contain the same variables so we can create a generic dataset...\n",
    "qs = qi_bins_c.shape[0]\n",
    "ts = T_bins_c.shape[0]\n",
    "ns = Ni_bins_c.shape[0]\n",
    "rs = RHi_bins_c.shape[0]\n",
    "\n",
    "ds_generic = xr.Dataset( \n",
    "    data_vars=dict(  qiTh=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),\n",
    "                     qiTh_outflow=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh_outflow=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih_outflow=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih_outflow=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),\n",
    "                     qiTh_insitu=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh_insitu=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih_insitu=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih_insitu=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),\n",
    "                     qiTh_flight=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh_flight=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih_flight=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih_flight=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c),\n",
    "                  T_bin=(['T_bin'], T_bins_c),\n",
    "                  RHi_bin=(['RHi_bin'], RHi_bins_c)) )\n",
    "\n",
    "# ... and then copy it 6 times.\n",
    "datasets = [ ds_generic , ds_generic.copy(), ds_generic.copy(), ds_generic.copy() ]#, ds_generic.copy(), ds_generic.copy() ]\n",
    "del ds_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ALL\n",
      "28.374138473829014 3.5058144104558497\n",
      "229.15626961105647 229.13717504411025\n",
      "OUTFLOW\n",
      "25.568006876296693 3.269421283663666\n",
      "228.43478788285128 228.65268802472406\n",
      "IN-SITU\n",
      "74.38580371911817 53.91527847335057\n",
      "208.99031519129284 209.31057515818677\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "ALL\n",
      "16.041046 1.9791634\n",
      "227.77415 230.31566\n",
      "OUTFLOW\n",
      "16.234705 3.3710952\n",
      "229.75185 230.26468\n",
      "IN-SITU\n",
      "0.016930597 0.0021297424\n",
      "194.98602 195.1895\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ALL\n",
      "63.64343383691187 12.066657283403261\n",
      "225.0807585987306 225.63447808381372\n",
      "OUTFLOW\n",
      "60.680852238291315 11.767587862906378\n",
      "224.94763642148988 225.49671962591077\n",
      "IN-SITU\n",
      "74.09154885207222 10.311170899430504\n",
      "207.71290911580462 208.35885302546984\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "ALL\n",
      "34.137196 0.096541524\n",
      "220.54869 227.62724\n",
      "OUTFLOW\n",
      "35.701603 1.8350012\n",
      "228.91061 230.05045\n",
      "IN-SITU\n",
      "0.69508284 0.0006339217\n",
      "198.06511 198.16257\n"
     ]
    }
   ],
   "source": [
    "#for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "for f, d, j in zip(fichiers, datasets, np.arange(6)):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi = fi['qi']\n",
    "    T = fi['T']\n",
    "    if (j != 1) & (j != 3):\n",
    "        P = fi['PE']\n",
    "    else:\n",
    "        P = fi['p']\n",
    "    time = fi['time']\n",
    "    qi_ppmv = calc_water( T, P, qi )\n",
    "    qi_ppmv.name = 'qi_ppmv'\n",
    "    # Ensure that qi [kg kg-1] is not mistakenly used below\n",
    "    del qi\n",
    "   \n",
    "\n",
    "    # Filter for non-negligible qi (or now CLaMS RHi) values; side-note: large peak in ICON qi at 10**(-10)\n",
    "    qi_ppmv = qi_ppmv.where( qi_ppmv > qi_threshold )\n",
    "    #qi = qi.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    #T = T.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    T = T.where( (qi_ppmv > qi_threshold) )\n",
    "    print('ALL')\n",
    "    print(np.nanmean(qi_ppmv), np.nanmedian(qi_ppmv))\n",
    "    print(np.nanmean(T), np.nanmedian(T))\n",
    "    \n",
    "    qi_ppmv_outflow = qi_ppmv.where( (T >= 210) & (T <= 237) )\n",
    "    T_outflow = T.where( (T >= 210) & (T <= 237) )\n",
    "    print('OUTFLOW')\n",
    "    print(np.nanmean(qi_ppmv_outflow), np.nanmedian(qi_ppmv_outflow))\n",
    "    print(np.nanmean(T_outflow), np.nanmedian(T_outflow))\n",
    "    \n",
    "    qi_ppmv_insitu = qi_ppmv.where( (T < 210) )\n",
    "    T_insitu = T.where( (T < 210) )\n",
    "    print('IN-SITU')\n",
    "    print(np.nanmean(qi_ppmv_insitu), np.nanmedian(qi_ppmv_insitu))\n",
    "    print(np.nanmean(T_insitu), np.nanmedian(T_insitu))\n",
    "    \n",
    "    #qi_flight = qi.sel( time=slice(time0, timef) )\n",
    "    #T_flight = T.sel( time=slice(time0, timef) )\n",
    "    #print('FLIGHT')\n",
    "    #print(np.nanmean(qi_flight), np.nanmedian(qi_flight))\n",
    "    #print(np.nanmean(T_flight), np.nanmedian(T_flight))\n",
    "    #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv, T, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv_outflow, T_outflow, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv_insitu, T_insitu, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh_insitu'] = h\n",
    "    \n",
    "    #wgts = 1*xr.apply_ufunc( np.isfinite, qi_flight )\n",
    "    #wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    #h = xhist( qi_flight, T_flight, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    #d['qiTh_flight'] = h\n",
    "    \n",
    "#allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ALL\n",
      "469.69815419267474 1.5138504554516892\n",
      "98.68632963850321 98.13186588831965\n",
      "OUTFLOW\n",
      "502.55247710372447 1.429200198443524\n",
      "98.78907840591017 98.09740781239327\n",
      "IN-SITU\n",
      "8526.863268527693 4.3834688560916767e-08\n",
      "42.013549919297205 33.16962527746901\n",
      "FLIGHT\n",
      "1656.4784070906421 2.378454299382606\n",
      "98.27127101031303 98.15169304863457\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "ICON 1-mom.. geht es weiter!\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ALL\n",
      "727.0428843756046 11.4090775282803\n",
      "100.08917528168654 99.44192003667722\n",
      "OUTFLOW\n",
      "692.8073463023833 11.25555648765884\n",
      "100.1783998125282 99.41351659863903\n",
      "IN-SITU\n",
      "3347.8617884472224 139.84646297952924\n",
      "97.19415248601527 98.74050399114871\n",
      "FLIGHT\n",
      "292.4046167128873 2.379296563631184\n",
      "101.61209433144593 99.96309977699333\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "ALL\n",
      "1120.5527 0.6783461\n",
      "70.34953 81.57728\n",
      "OUTFLOW\n",
      "1474.9387 4.5431542\n",
      "88.55375 92.29458\n",
      "IN-SITU\n",
      "56.68363 0.014598131\n",
      "32.737934 28.254412\n",
      "FLIGHT\n",
      "33.935944 0.2863234\n",
      "72.88132 83.16351\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CPU times: user 1min 51s, sys: 33.3 s, total: 2min 24s\n",
      "Wall time: 11min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the Nih_* values in the Datasets above.\n",
    "#for f, d, r, j in zip(fichiers, datasets, RHi_fi, np.arange(6)):\n",
    "for f, d, j in zip(fichiers, datasets, np.arange(4)):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    if j == 1:\n",
    "        print('ICON 1-mom.. geht es weiter!')\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        continue\n",
    "    else:\n",
    "        # Convert kg-1 to L-1\n",
    "        rho = fi['rho']\n",
    "        Ni = fi['Ni'] * rho  / 1000.\n",
    "        Ni.name = fi['Ni'].name\n",
    "        T = fi['T']\n",
    "        RHI = fi['RHI']\n",
    "        time = fi['time']\n",
    "    \n",
    "        # Filter for non-negligible values\n",
    "        #Ni = Ni.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "        #RHI = RHI.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "        Ni = Ni.where( (Ni > Ni_threshold) )\n",
    "        RHI = RHI.where( (Ni > Ni_threshold) )\n",
    "        print('ALL')\n",
    "        print(np.nanmean(Ni), np.nanmedian(Ni))\n",
    "        print(np.nanmean(RHI), np.nanmedian(RHI))\n",
    "    \n",
    "        Ni_outflow = Ni.where( (T >= 210) & (T <= 237) )\n",
    "        RHI_outflow = RHI.where( (T >= 210) & (T <= 237) )\n",
    "        print('OUTFLOW')\n",
    "        print(np.nanmean(Ni_outflow), np.nanmedian(Ni_outflow))\n",
    "        print(np.nanmean(RHI_outflow), np.nanmedian(RHI_outflow))\n",
    "    \n",
    "        Ni_insitu = Ni.where( (T < 210) )\n",
    "        RHI_insitu = RHI.where( (T < 210) )\n",
    "        print('IN-SITU')\n",
    "        print(np.nanmean(Ni_insitu), np.nanmedian(Ni_insitu))\n",
    "        print(np.nanmean(RHI_insitu), np.nanmedian(RHI_insitu))\n",
    "    \n",
    "        Ni_flight = Ni.sel( time=slice(time0, timef) )\n",
    "        RHI_flight = RHI.sel( time=slice(time0, timef) )\n",
    "        print('FLIGHT')\n",
    "        print(np.nanmean(Ni_flight), np.nanmedian(Ni_flight))\n",
    "        print(np.nanmean(RHI_flight), np.nanmedian(RHI_flight))\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "        # Weight by the total number of non-nan values\n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "        # Weight by the total number of non-nan values\n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni, RHI, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "        d['NiRHih'] = h\n",
    "    \n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni_outflow )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni_outflow, RHI_outflow, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "        d['NiRHih_outflow'] = h\n",
    "    \n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni_insitu )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni_insitu, RHI_insitu, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "        d['NiRHih_insitu'] = h\n",
    "    \n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni_flight )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni_flight, RHI_flight, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "        d['NiRHih_flight'] = h\n",
    "    \n",
    "#allDone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ALL\n",
      "28.374138473829014 3.5058144104558497\n",
      "98.66700142995818 98.12607656654461\n",
      "OUTFLOW\n",
      "25.568006876296693 3.269421283663666\n",
      "98.75754321251502 98.08903778804782\n",
      "INSITU\n",
      "74.38580371911817 53.91527847335057\n",
      "95.38207725737043 95.23517238808995\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "ALL\n",
      "16.041046 1.9791634\n",
      "88.83188 95.1699\n",
      "OUTFLOW\n",
      "16.234705 3.3710952\n",
      "92.471176 96.38266\n",
      "INSITU\n",
      "0.016930597 0.0021297424\n",
      "53.251255 51.426132\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ALL\n",
      "63.64343383691187 12.066657283403261\n",
      "100.069822391198 99.4382955233491\n",
      "OUTFLOW\n",
      "60.680852238291315 11.767587862906378\n",
      "100.15082649436627 99.40907335954074\n",
      "INSITU\n",
      "74.09154885207222 10.311170899430504\n",
      "97.49790142950933 98.76648876301174\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "ALL\n",
      "34.137196 0.096541524\n",
      "71.002975 81.56012\n",
      "OUTFLOW\n",
      "35.701603 1.8350012\n",
      "87.946045 91.850975\n",
      "INSITU\n",
      "0.69508284 0.0006339217\n",
      "33.484924 29.160877\n"
     ]
    }
   ],
   "source": [
    "#for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "for f, d, j in zip(fichiers, datasets,np.arange(6)):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi = fi['qi']\n",
    "    T = fi['T']\n",
    "    RHI = fi['RHI']\n",
    "    if (j != 1) & (j != 3):\n",
    "        P = fi['PE']\n",
    "    else:\n",
    "        P = fi['p']\n",
    "    time = fi['time']\n",
    "    qi_ppmv = calc_water( T, P, qi )\n",
    "    qi_ppmv.name = 'qi_ppmv'\n",
    "    # Ensure that qi [kg kg-1] is not mistakenly used below\n",
    "    del qi\n",
    "\n",
    "    # Filter for non-negligible qi (or now CLaMS RHi) values; side-note: large peak in ICON qi at 10**(-10)\n",
    "    qi_ppmv = qi_ppmv.where( qi_ppmv > qi_threshold )\n",
    "    #qi = qi.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    #RHI = RHI.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    RHI = RHI.where( (qi_ppmv > qi_threshold) )\n",
    "    print('ALL')\n",
    "    print(np.nanmean(qi_ppmv), np.nanmedian(qi_ppmv))\n",
    "    print(np.nanmean(RHI), np.nanmedian(RHI))\n",
    "    \n",
    "    qi_ppmv_outflow = qi_ppmv.where( (T >= 210) & (T <= 237) )\n",
    "    RHI_outflow = RHI.where( (T >= 210) & (T <= 237) )\n",
    "    print('OUTFLOW')\n",
    "    print(np.nanmean(qi_ppmv_outflow), np.nanmedian(qi_ppmv_outflow))\n",
    "    print(np.nanmean(RHI_outflow), np.nanmedian(RHI_outflow))\n",
    "    \n",
    "    qi_ppmv_insitu = qi_ppmv.where( (T < 210) )\n",
    "    RHI_insitu = RHI.where( (T < 210) )\n",
    "    print('INSITU')\n",
    "    print(np.nanmean(qi_ppmv_insitu), np.nanmedian(qi_ppmv_insitu))\n",
    "    print(np.nanmean(RHI_insitu), np.nanmedian(RHI_insitu))\n",
    "    \n",
    "    #qi_ppmv_flight = qi_ppmv.sel( time=slice(time0, timef) )\n",
    "    #RHI_flight = RHI.sel( time=slice(time0, timef) )\n",
    "    #print('FLIGHT')\n",
    "    #print(np.nanmean(qi_ppmv_flight), np.nanmedian(qi_ppmv_flight))\n",
    "    #print(np.nanmean(RHI_flight), np.nanmedian(RHI_flight))\n",
    "    #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~`')\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv, RHI, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv_outflow, RHI_outflow, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_ppmv_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_ppmv_insitu, RHI_insitu, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih_insitu'] = h\n",
    "    \n",
    "    #wgts = 1*xr.apply_ufunc( np.isfinite, qi_flight )\n",
    "    #wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    #h = xhist( qi_flight, RHI_flight, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    #d['qiRHih_flight'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V1M0A0R_tst00001350_trim_extract_dt_filter.nc\n",
      "ICON 1-mom.. geht es weiter!\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc_filter.nc\n",
      "ICON_0V2M0A0R_tst00000450_trim_extract_dt_filter.nc\n",
      "CPU times: user 1min 14s, sys: 25.1 s, total: 1min 39s\n",
      "Wall time: 7min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the Nih_* values in the Datasets above.\n",
    "#for f, d, r, j in zip(fichiers, datasets, RHi_fi, np.arange(6)):\n",
    "for f, d, j in zip(fichiers, datasets, np.arange(4)):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    if j == 1:\n",
    "        print('ICON 1-mom.. geht es weiter!')\n",
    "        print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "        continue\n",
    "    else:\n",
    "        # Convert kg-1 to L-1\n",
    "        rho = fi['rho']\n",
    "        Ni = fi['Ni'] * rho  / 1000.\n",
    "        Ni.name = fi['Ni'].name\n",
    "        T = fi['T']\n",
    "        time = fi['time']\n",
    "    \n",
    "        # Filter for non-negligible values\n",
    "        #Ni = Ni.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "        #T = T.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "        Ni = Ni.where( (Ni > Ni_threshold) )\n",
    "        T = T.where( (Ni > Ni_threshold) )\n",
    "    \n",
    "        Ni_outflow = Ni.where( (T >= 210) & (T <= 237) )\n",
    "        T_outflow = T.where( (T >= 210) & (T <= 237) )\n",
    "    \n",
    "        Ni_insitu = Ni.where( (T < 210) )\n",
    "        T_insitu = T.where( (T < 210) )\n",
    "    \n",
    "        Ni_flight = Ni.sel( time=slice(time0, timef) )\n",
    "        T_flight = T.sel( time=slice(time0, timef) )\n",
    "\n",
    "        # Weight by the total number of non-nan values\n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni, T, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "        d['NiTh'] = h\n",
    "    \n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni_outflow )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni_outflow, T_outflow, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "        d['NiTh_outflow'] = h\n",
    "    \n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni_insitu )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni_insitu, T_insitu, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "        d['NiTh_insitu'] = h\n",
    "    \n",
    "        wgts = 1*xr.apply_ufunc( np.isfinite, Ni_flight )\n",
    "        wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "        h = xhist( Ni_flight, T_flight, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "        d['NiTh_flight'] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qi-Nih-T-RHi 2D histograms saved to nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d, j in zip(datasets, np.arange(6)):\n",
    "    d['qiTh'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus temperature over [qi_bin, T_bin]\" \n",
    "    d['qiTh'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiTh'].attrs['description'] = \"2D hist values of ice crystal number concentration versus temperature over [Ni_bin, T_bin]\" \n",
    "    d['NiTh'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qiTh_outflow'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus temperature for T >= 210 K over [qi_bin, T_bin]\" \n",
    "    d['qiTh_outflow'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiTh_outflow'].attrs['description'] = \"2D hist values of ice crystal number concentration versus temperature for T >= 210 K over [Ni_bin, T_bin]\" \n",
    "    d['NiTh_outflow'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qiTh_insitu'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus temperature for T < 210 K over [qi_bin, T_bin]\" \n",
    "    d['qiTh_insitu'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiTh_insitu'].attrs['description'] = \"2D hist values of ice crystal number concentration versus temperature for T < 210 K over [Ni_bin, T_bin]\" \n",
    "    d['NiTh_insitu'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qiTh_flight'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus temperature for StratoClim Flight 7 times over [qi_bin, T_bin]\" \n",
    "    d['qiTh_flight'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiTh_flight'].attrs['description'] = \"2D hist values of ice crystal number concentration versus temperature for StratoClim Flight 7 times over [Ni_bin, T_bin]\" \n",
    "    d['NiTh_flight'].attrs['units'] = \"Probability [%]\"\n",
    "    \n",
    "    d['qiRHih'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus rel hum wrt ice over [qi_bin, RHi_bin]\" \n",
    "    d['qiRHih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiRHih'].attrs['description'] = \"2D hist values of ice crystal number concentration versus rel hum wrt ice over [Ni_bin, RHi_bin]\" \n",
    "    d['NiRHih'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qiRHih_outflow'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus rel hum wrt ice for T >= 210 K over [qi_bin, RHi_bin]\" \n",
    "    d['qiRHih_outflow'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiRHih_outflow'].attrs['description'] = \"2D hist values of ice crystal number concentration versus rel hum wrt ice for T >= 210 K over [Ni_bin, RHi_bin]\" \n",
    "    d['NiRHih_outflow'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qiRHih_insitu'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus rel hum wrt ice for T < 210 K over [qi_bin, RHi_bin]\" \n",
    "    d['qiRHih_insitu'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiRHih_insitu'].attrs['description'] = \"2D hist values of ice crystal number concentration versus rel hum wrt ice for T < 210 K over [Ni_bin, RHi_bin]\" \n",
    "    d['NiRHih_insitu'].attrs['units'] = \"Probability [%]\"\n",
    "    d['qiRHih_flight'].attrs['description'] = \"2D hist values of ice mass mixing ratio versus rel hum wrt ice for StratoClim Flight 7 times over [qi_bin, T_bin]\" \n",
    "    d['qiRHih_flight'].attrs['units'] = \"Probability [%]\"\n",
    "    d['NiRHih_flight'].attrs['description'] = \"2D hist values of ice crystal number concentration versus rel hum wrt ice for StratoClim Flight 7 times over [Ni_bin, T_bin]\" \n",
    "    d['NiRHih_flight'].attrs['units'] = \"Probability [%]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "writedir = '/xdisk/sylvia/traj_output/traj_pp/'\n",
    "names = [ 'qippmvNi-TRHi-2Dh-CLAMS-Tf_0V1M0A0R.nc', 'qippmvNi-TRHi-2Dh-ICON_0V1M0A0R.nc', \n",
    "          'qippmvNi-TRHi-2Dh-CLAMS-Tf_0V2M0A0R.nc', 'qippmvNi-TRHi-2Dh-ICON_0V2M0A0R.nc',\n",
    "          'qippmvNi-TRHi-2Dh-CLAMS_0V2M0A0R.nc', 'qippmvNi-TRHi-2Dh-CLAMS-Tf_0V2M0A0R_noSHflux.nc' ]\n",
    "for n, d in zip(names, datasets):\n",
    "    d.to_netcdf( writedir + n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncplot",
   "language": "python",
   "name": "ncplot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

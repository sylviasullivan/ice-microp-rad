{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from xhistogram.xarray import histogram as xhist\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(\"/work/bb1018/b380873/tropic_vis/utilities/\"))\n",
    "from plotting_utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cdo commands to adjust the ICON nc variable names and contents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "cdo chname,t,T,qni,Ni ICON_file.nc ICON_file2.nc\n",
    "cdo merge -expr,'psati=exp(9.550426-5723.265/T+3.53068*log(T)-0.00728332*T)' ICON_file2.nc ICON_file2.nc ICON_file3.nc\n",
    "cdo merge -expr,'RHI=qv*100/0.622/psati*(p-psati)' ICON_file3.nc ICON_fil3.nc ICON_file4.nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all ICON trajectories into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the ICON-1M trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_ICON_0V1M0A0R/'\n",
    "new_file = xr.open_dataset( basedir + 'traj_tst00001350_p001_trim_extract_dt.nc' )\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'traj_tst00001350_p' + traj_prefix(i) + str(i) + '_trim_extract_dt.nc' ) # ICON\n",
    "    new_file = xr.concat( (new_file, file2), dim='id' )\n",
    "    \n",
    "display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'traj_tst00001350_trim_extract_dt.nc' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the ICON-2M trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_ICON_0V2M0A0R/'\n",
    "new_file = xr.open_dataset( basedir + 'traj_tst00000450_p001_trim_extract_dt.nc' )\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'traj_tst00000450_p' + traj_prefix(i) + str(i) + '_trim_extract_dt.nc' ) # ICON\n",
    "    new_file = xr.concat( (new_file, file2), dim='id' )\n",
    "    \n",
    "display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'traj_tst00000450_trim_extract_dt.nc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cdo commands to adjust the CLaMS nc variables names and contents"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "for i in $(ls cirrus_tst*); do cdo settaxis,2017-08-06,09:00:00,24sec $i ${i%'.nc'}'_dt.nc'; done\n",
    "for i in $(ls cirrus_tst*dt.nc); do cdo merge -expr,'qi=IWC_het+IWC_hom+IWC_pre;Ni=ICN_pre+ICN_het+ICN_hom' -chname,RHO,rho $i $i ${i%'.nc'}'_iwc.nc'; done\n",
    "module load nco/4.7.5-gcc64\n",
    "for i in $(ls cirrus_tst*iwc.nc); do ncrename -d NPARTS,id $i ${i%'.nc'}'2.nc'; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all CLaMS trajectores into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all of the CLAMS trajectory files into one\n",
    "basedir = '/work/bb1018/b380873/traj_output/traj_CLAMS-Tf_0V2M0A0R_noSHflux/'\n",
    "new_file = xr.open_dataset( basedir + 'cirrus_tst00000450_p001_trim_clams_dt_iwc.nc' ) #extract_\n",
    "for i in np.arange(2,27):\n",
    "    print(i)\n",
    "    file2 = xr.open_dataset( basedir + 'cirrus_tst00000450_p' + traj_prefix(i) + str(i) + '_trim_clams_dt_iwc.nc' ) # extract_\n",
    "    new_file = xr.concat( (new_file, file2), dim='id' )\n",
    "    \n",
    "#display(new_file)\n",
    "new_file.to_netcdf( path=basedir + 'cirrus_tst00000450_trim_clams_dt_iwc.nc' ) # extract_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all trajectories for 5 simulations from 5 files\n",
    "Initiate the binning for different variables here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# These thresholds are used throughout. Values below are considered negligible.\n",
    "qi_threshold = 10**(-8)\n",
    "Ni_threshold = 10**(-8)\n",
    "RHi_threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "basedir = '/work/bb1018/b380873/traj_output/'\n",
    "fichiers = [ 'CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc' ]\n",
    "            #'CLAMS-Tf_0V1M0A0R_tst00001350_trim_extract_dt_iwc.nc', 'ICON_0V1M0A0R_tst00001350_trim_extract_dt.nc',\n",
    "            # 'CLAMS-Tf_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc', 'ICON_0V2M0A0R_tst00000450_trim_extract_dt.nc',\n",
    "            # 'CLAMS_0V2M0A0R_tst00000450_trim_extract_dt_iwc.nc']\n",
    "\n",
    "# Logarithmic IWC bins in ppmv, as well as their centerpoints\n",
    "qi_bins = np.logspace( -5, 3.5, 50 )\n",
    "qi_bins_c = ( qi_bins[1:] + qi_bins[:-1] )/2.\n",
    "\n",
    "# Logarithmic Ni bins in cm-3, as well as their centerpoints\n",
    "Ni_bins = np.logspace( -4, 5.5, 50 )\n",
    "Ni_bins_c = ( Ni_bins[1:] + Ni_bins[:-1] )/2.\n",
    "\n",
    "# Linear T bins in K\n",
    "T_bins = np.linspace( 190, 240, 50 )\n",
    "T_bins_c = ( T_bins[1:] + T_bins[:-1] )/2.\n",
    "\n",
    "# Linear RHi bins in %\n",
    "RHi_bins = np.linspace( 60, 120, 50 )\n",
    "RHi_bins_c = ( RHi_bins[1:] + RHi_bins[:-1] )/2.\n",
    "\n",
    "# Flight 7 track times\n",
    "time0 = datetime(2017, 8, 8, 6, 20)\n",
    "timef = datetime(2017, 8, 8, 6, 48)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a generically structured dataset to store histogram values and copy it 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Initially all the datasets contain the same variables so we can create a generic dataset...\n",
    "ds_generic = xr.Dataset( \n",
    "    data_vars=dict(  qih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     Th=(['T_bin'], np.zeros(T_bins_c.shape)),\n",
    "                     qih_outflow=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih_outflow=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qih_insitu=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih_insitu=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qih_flight=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nih_flight=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     RHih=(['RHi_bin'], np.zeros(RHi_bins_c.shape)) ),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c),\n",
    "                  T_bin=(['T_bin'], T_bins_c),\n",
    "                  RHi_bin=(['RHi_bin'], RHi_bins_c) ) )\n",
    "\n",
    "# ... and then copy it 5 times.\n",
    "datasets =  [ ds_generic ]\n",
    "#[ ds_generic, ds_generic.copy(), ds_generic.copy(), ds_generic.copy(), ds_generic.copy() ]\n",
    "del ds_generic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different 'flavors' of qi and Ni histograms calculated and mean / median qi / Ni printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#RHi_1M = xr.open_dataset( basedir + fichiers[0] )['RHI']\n",
    "RHi_2M = xr.open_dataset( basedir + fichiers[0] )['RHI']  # fichiers[2]\n",
    "RHi_fi = [ RHi_2M ]\n",
    "    #RHi_1M, RHi_1M, RHi_2M, RHi_2M, RHi_2M ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "53.43395037211612 16.62167596805375\n",
      "52.032821274506155 16.779730685811955\n",
      "36.62998601484411 9.196852261084132\n",
      "14.768917712764654 5.806602985103382\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CPU times: user 46.3 s, sys: 37.3 s, total: 1min 23s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the qih_* values in the Datasets above\n",
    "for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi = fi['qi'] * 10**6\n",
    "    T = fi['T']\n",
    "    time = fi['time']\n",
    "\n",
    "    # Filter for non-negligible qi (or now CLaMS RHi) values; side-note: large peak in ICON qi at 10**(-10)\n",
    "    #qi = qi.where( qi > qi_threshold )\n",
    "    qi = qi.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    qi_outflow = qi.where( (T >= 210) & (T <= 237) )\n",
    "    qi_insitu = qi.where( (T < 210) )\n",
    "    qi_flight = qi.sel( time=slice(time0, timef) )\n",
    "    print(np.nanmean(qi),np.nanmedian(qi))\n",
    "    print(np.nanmean(qi_outflow),np.nanmedian(qi_outflow))\n",
    "    print(np.nanmean(qi_insitu),np.nanmedian(qi_insitu))\n",
    "    print(np.nanmean(qi_flight),np.nanmedian(qi_flight))\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_outflow, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_insitu, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih_insitu'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_flight )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_flight, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qih_flight'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, T )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( T, dim=['time','id'], block_size=100, weights=wgts, bins=[T_bins] )\n",
    "    d['Th'] = h\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "536.1990738865144 24.284928057732714\n",
      "522.1387132623357 23.95665650941671\n",
      "1575.3982748179615 61.09877820867155\n",
      "242.30756702692958 11.026734980206898\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "CPU times: user 37.1 s, sys: 34.4 s, total: 1min 11s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Save the Nih_* values in the Datasets above.\n",
    "for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg-1 to L-1\n",
    "    rho = fi['RHO']\n",
    "    Ni = fi['Ni'] * rho  / 1000.\n",
    "    Ni.name = fi['Ni'].name\n",
    "    T = fi['T']\n",
    "    time = fi['time']\n",
    "    \n",
    "    # Filter for non-negligible values\n",
    "    Ni = Ni.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "    Ni_outflow = Ni.where( (T >= 210) & (T <= 237) )\n",
    "    Ni_insitu = Ni.where( (T < 210) )\n",
    "    Ni_flight = Ni.sel( time=slice(time0, timef) )\n",
    "    print(np.nanmean(Ni),np.nanmedian(Ni))\n",
    "    print(np.nanmean(Ni_outflow),np.nanmedian(Ni_outflow))\n",
    "    print(np.nanmean(Ni_insitu),np.nanmedian(Ni_insitu))\n",
    "    print(np.nanmean(Ni_flight),np.nanmedian(Ni_flight))\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_outflow, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_insitu, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih_insitu'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_flight )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_flight, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nih_flight'] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TE and RHI histograms calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n",
      "CPU times: user 29 s, sys: 30.5 s, total: 59.5 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "basedir = '/work/bb1018/b380873/traj_output/'\n",
    "# Save the RHIh_ values for both datasets and the TEh_* values for the CLaMS Datasets only\n",
    "for f, d in zip(fichiers, datasets):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    qi = fi['qi'] * 10**6\n",
    "    RHi = fi['RHI'].where( qi > qi_threshold )\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, RHi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( RHi, dim=['time','id'], block_size=100, weights=wgts, bins=[RHi_bins] )\n",
    "    d['RHih'] = h\n",
    "\n",
    "for f, d in zip(fichiers, datasets):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    qi = fi['qi'] * 10**6\n",
    "    TE = fi['TE'].where( qi > qi_threshold )\n",
    "    RHi = fi['RHI'].where( qi > qi_threshold )\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, TE )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( TE, dim=['time','id'], block_size=100, weights=wgts, bins=[T_bins] )\n",
    "    d['TEh'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, RHi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( RHi, dim=['time','id'], block_size=100, weights=wgts, bins=[RHi_bins] )\n",
    "    d['RHih'] = h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### qi-Nih-T-RHi histograms saved to nc files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Save the qi and Ni histogram datasets in nc files\n",
    "writedir = '/work/bb1018/b380873/traj_output/traj_pp/'\n",
    "names = [ 'qih-Nih-CLAMS-Tf_0V2M0A0R_noSHflux.nc' ]\n",
    "    #'qih-Nih-CLAMS-Tf_0V1M0A0R2.nc', 'qih-Nih-ICON_0V1M0A0R2.nc', \n",
    "    #      'qih-Nih-CLAMS-Tf_0V2M0A0R2.nc', 'qih-Nih-ICON_0V2M0A0R2.nc',\n",
    "    #      'qih-Nih-CLAMS_0V2M0A0R2.nc']\n",
    "for n, d in zip(names, datasets):\n",
    "    d.to_netcdf( writedir + n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ice microphysical process histograms\n",
    "These differ between CLaMS and ICON so we have to initially create two different structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CLaMS, we will look at the homogeneous, heterogeneous, and preexisiting qi and Ni\n",
    "ds_clams = xr.Dataset( \n",
    "    data_vars=dict(  qhomih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nhomih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qhetih=(['qi_bin'], np.zeros(T_bins_c.shape)),\n",
    "                     Nhetih=(['Ni_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     qpreih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Npreih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)) ),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c) ) )\n",
    "\n",
    "# ... and then copy it 3 times.\n",
    "datasets_clams = [ ds_clams, ds_clams.copy(), ds_clams.copy() ]\n",
    "del ds_clams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For ICON, we will look at qi and Ni sedimentation fluxes in and out\n",
    "ds_icon = xr.Dataset( \n",
    "    data_vars=dict(  qsedih=(['qi_bin'], np.zeros(qi_bins_c.shape)),\n",
    "                     Nsedih=(['Ni_bin'], np.zeros(Ni_bins_c.shape)),\n",
    "                     qsedoh=(['qi_bin'], np.zeros(T_bins_c.shape)),\n",
    "                     Nsedoh=(['Ni_bin'], np.zeros(qi_bins_c.shape)) ),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c) ) )\n",
    "\n",
    "# ... and then copy it 3 times.\n",
    "datasets_icon = [ ds_icon, ds_icon.copy(), ds_icon.copy() ]\n",
    "del ds_icon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAMS-Tf_0V2M0A0R_noSHflux_tst00000450_trim_extract_dt_iwc.nc\n"
     ]
    }
   ],
   "source": [
    "basedir = '/work/bb1018/b380873/traj_output/'\n",
    "for f, d in zip(fichiers, datasets_clams):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert to ppmv / L-1 and filter for non-negligible values\n",
    "    qi_hom = fi['IWC_hom'] * 10**6\n",
    "    qi_hom = qi_hom.where( qi_hom > qi_threshold )\n",
    "    \n",
    "    qi_het = fi['IWC_het'] * 10**6\n",
    "    qi_het = qi_het.where( qi_het > qi_threshold )\n",
    "    \n",
    "    qi_pre = fi['IWC_pre'] * 10**6\n",
    "    qi_pre = qi_pre.where( qi_pre > qi_threshold )\n",
    "    \n",
    "    rho = fi['RHO'] # rho\n",
    "    Ni_hom = fi['ICN_hom'] * rho / 1000\n",
    "    Ni_hom.name = fi['ICN_hom'].name\n",
    "    Ni_hom = Ni_hom.where( Ni_hom > Ni_threshold )\n",
    "    \n",
    "    Ni_het = fi['ICN_het'] * rho / 1000\n",
    "    Ni_het.name = fi['ICN_het'].name\n",
    "    Ni_het = Ni_hom.where( Ni_het > Ni_threshold )\n",
    "    \n",
    "    Ni_pre = fi['ICN_pre'] * rho / 1000\n",
    "    Ni_pre.name = fi['ICN_pre'].name\n",
    "    Ni_pre = Ni_pre.where( Ni_pre > Ni_threshold )\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_hom )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_hom, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qhomih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_het )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_het, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qhetih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_pre )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_pre, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qpreih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_hom )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_hom, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nhomih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_het )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_het, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Nhetih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_pre )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_pre, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['Npreih'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "basedir = '/work/bb1018/b380873/traj_output/'\n",
    "# Save the <process>h_ values for both datasets, ICON first\n",
    "for f, d in zip(fichiers[1::2], datasets_icon):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert to ppmv and filter for non-negligible values\n",
    "    # We want the magnitude of these sedimentation values so multiply by -1.\n",
    "    qsedi = -1*fi['qsedi'] * 10**6\n",
    "    qsedi = qsedi.where( qsedi > qi_threshold )\n",
    "    qsedo = -1*fi['qsedo'] * 10**6\n",
    "    qsedo = qsedo.where( qsedo > qi_threshold )\n",
    "    \n",
    "    # As above but converting to L-1\n",
    "    rho = fi['rho']\n",
    "    qnsedi = fi['qnsedi'] * rho / 1000.\n",
    "    qnsedi.name = fi['qnsedi'].name\n",
    "    qnsedi = -1*qnsedi.where( qnsedi > Ni_threshold )               \n",
    "    qnsedo = fi['qnsedo'] * rho / 1000.\n",
    "    qnsedo.name = fi['qnsedo'].name\n",
    "    qnsedo = -1*qnsedi.where( qnsedo > Ni_threshold )    \n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qsedi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qsedi, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qsedih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qsedo )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qsedo, dim=['time','id'], block_size=100, weights=wgts, bins=[qi_bins] )\n",
    "    d['qsedoh'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qnsedi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qnsedi, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['qnsedih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qnsedo )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qnsedo, dim=['time','id'], block_size=100, weights=wgts, bins=[Ni_bins] )\n",
    "    d['qnsedoh'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scratchdir = '/work/bb1018/b380873/traj_output/traj_pp/'\n",
    "names = [ 'het-hom-pre-CLAMS-Tf_0V2M0A0R_noSHflux.nc' ]\n",
    "    #'het-hom-pre-CLAMS-Tf_0V1M0A0R.nc', 'qsed-ICON_0V1M0A0R.nc', \n",
    "    #      'het-hom-pre-CLAMS-Tf_0V2M0A0R.nc', 'qsed-ICON_0V2M0A0R.nc',\n",
    "    #      'het-hom-pre-CLAMS_0V2M0A0R.nc']\n",
    "datasets = [ datasets_clams[0], datasets_icon[0], datasets_clams[1],\n",
    "             datasets_icon[1], datasets_clams[2] ]\n",
    "for n, d in zip(names, datasets):\n",
    "    d.to_netcdf( scratchdir + n )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-dimensional histograms - qi, Ni versus T, RHi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the number of qi and Ni bins relative to the 1D histograms\n",
    "\n",
    "# Logarithmic IWC bins in ppmv, as well as their centerpoints\n",
    "qi_bins = np.logspace( -5, 3.5, 25 )\n",
    "qi_bins_c = ( qi_bins[1:] + qi_bins[:-1] )/2.\n",
    "\n",
    "# Logarithmic Ni bins in cm-3, as well as their centerpoints\n",
    "Ni_bins = np.logspace( -4, 5.5, 25 )\n",
    "Ni_bins_c = ( Ni_bins[1:] + Ni_bins[:-1] )/2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using StratoClim data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = xr.open_dataset( '/work/bb1018/b380873/tropic_vis/obs/stratoclim2017.geophysika.0808_1.filtered_per_sec.nc')\n",
    "sc_qi = sc['BEST:IWC']\n",
    "sc_temp = sc['BEST:TEMP']\n",
    "\n",
    "sc_qi = sc_qi.where( (sc_qi > qi_threshold) )\n",
    "sc_temp = sc_temp.where( (sc_qi > qi_threshold) )\n",
    "\n",
    "wgts = 1*xr.apply_ufunc( np.isfinite, sc_qi )\n",
    "wgts = wgts / wgts.sum( dim=['time'] ) * 100.\n",
    "\n",
    "h = xhist( sc_qi, sc_temp, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "qs = qi_bins_c.shape[0]\n",
    "ts = T_bins_c.shape[0]\n",
    "ds_generic = xr.Dataset( data_vars=dict( qiTh_flight=(['qi_bin','T_bin'], np.zeros((qs, ts)) ) ) )\n",
    "ds_generic['qiTh_flight'] = h\n",
    "ds_generic.to_netcdf( '/work/bb1018/b380873/tropic_vis/obs/stratoclim2017.geophysika.0808_1_hist.nc' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using simulation trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially all the datasets contain the same variables so we can create a generic dataset...\n",
    "qs = qi_bins_c.shape[0]\n",
    "ts = T_bins_c.shape[0]\n",
    "ns = Ni_bins_c.shape[0]\n",
    "rs = RHi_bins_c.shape[0]\n",
    "\n",
    "ds_generic = xr.Dataset( \n",
    "    data_vars=dict(  qiTh=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),\n",
    "                     qiTh_outflow=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh_outflow=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih_outflow=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih_outflow=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),\n",
    "                     qiTh_insitu=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh_insitu=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih_insitu=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih_insitu=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),\n",
    "                     qiTh_flight=(['qi_bin','T_bin'], np.zeros(( qs, ts )) ),\n",
    "                     NiTh_flight=(['Ni_bin','T_bin'], np.zeros(( ns, ts )) ),\n",
    "                     qiRHih_flight=(['qi_bin','RHi_bin'], np.zeros(( qs, rs )) ),\n",
    "                     NiRHih_flight=(['Ni_bin','RHi_bin'], np.zeros(( ns, rs )) ),),\n",
    "    coords=dict(  qi_bin=(['qi_bin'], qi_bins_c),\n",
    "                  Ni_bin=(['Ni_bin'], Ni_bins_c),\n",
    "                  T_bin=(['T_bin'], T_bins_c),\n",
    "                  RHi_bin=(['RHi_bin'], RHi_bins_c)) )\n",
    "\n",
    "# ... and then copy it 5 times.\n",
    "datasets = [ ds_generic, ds_generic.copy(), ds_generic.copy(), ds_generic.copy(), ds_generic.copy() ]\n",
    "del ds_generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi = fi['qi'] * 10**6\n",
    "    T = fi['T']\n",
    "    time = fi['time']\n",
    "\n",
    "    # Filter for non-negligible qi (or now CLaMS RHi) values; side-note: large peak in ICON qi at 10**(-10)\n",
    "    #qi = qi.where( qi > qi_threshold )\n",
    "    qi = qi.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    T = T.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    #print('ALL')\n",
    "    #print(qi.mean().values,T.mean().values)\n",
    "    #print(qi.median().values,T.median().values)\n",
    "    \n",
    "    qi_outflow = qi.where( (T >= 210) & (T <= 237) )\n",
    "    T_outflow = T.where( (T >= 210) & (T <= 237) )\n",
    "    #print('OUTFLOW')\n",
    "    #print(qi_outflow.mean().values,T_outflow.mean().values)\n",
    "    #print(qi_outflow.median().values,T_outflow.median().values)\n",
    "    \n",
    "    qi_insitu = qi.where( (T < 210) )\n",
    "    T_insitu = T.where( (T < 210) )\n",
    "    #print('IN-SITU')\n",
    "    #print(qi_insitu.mean().values,T_insitu.mean().values)\n",
    "    #print(qi_insitu.median().values,T_insitu.median().values)\n",
    "    \n",
    "    qi_flight = qi.sel( time=slice(time0, timef) )\n",
    "    T_flight = T.sel( time=slice(time0, timef) )\n",
    "    #print('FLIGHT')\n",
    "    #print(qi_flight.mean().values,T_flight.mean().values)\n",
    "    #print(qi_flight.median().values,T_flight.median().values)\n",
    "\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi, T, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_outflow, T_outflow, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_insitu, T_insitu, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh_insitu'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_flight )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_flight, T_flight, block_size=100, bins=[qi_bins, T_bins], weights=wgts )\n",
    "    d['qiTh_flight'] = h\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg kg-1 to ppmv\n",
    "    qi = fi['qi'] * 10**6\n",
    "    RHI = fi['RHI']\n",
    "    T = fi['T']\n",
    "    time = fi['time']\n",
    "\n",
    "    # Filter for non-negligible qi (or now CLaMS RHi) values; side-note: large peak in ICON qi at 10**(-10)\n",
    "    #qi = qi.where( qi > qi_threshold )\n",
    "    qi = qi.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    RHI = RHI.where( (r > RHi_threshold ) & (qi > qi_threshold) )\n",
    "    \n",
    "    qi_outflow = qi.where( (T >= 210) & (T <= 237) )\n",
    "    RHI_outflow = RHI.where( (T >= 210) & (T <= 237) )\n",
    "    \n",
    "    qi_insitu = qi.where( (T < 210) )\n",
    "    RHI_insitu = RHI.where( (T < 210) )\n",
    "    \n",
    "    qi_flight = qi.sel( time=slice(time0, timef) )\n",
    "    RHI_flight = RHI.sel( time=slice(time0, timef) )\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi, RHI, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_outflow, RHI_outflow, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_insitu, RHI_insitu, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih_insitu'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi_flight )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( qi_flight, RHI_flight, block_size=100, bins=[qi_bins, RHi_bins], weights=wgts )\n",
    "    d['qiRHih_flight'] = h\n",
    "    print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the Nih_* values in the Datasets above.\n",
    "for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg-1 to L-1\n",
    "    rho = fi['rho']\n",
    "    Ni = fi['Ni'] * rho  / 1000.\n",
    "    Ni.name = fi['Ni'].name\n",
    "    T = fi['T']\n",
    "    time = fi['time']\n",
    "    \n",
    "    # Filter for non-negligible values\n",
    "    Ni = Ni.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "    T = T.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "    \n",
    "    Ni_outflow = Ni.where( (T >= 210) & (T <= 237) )\n",
    "    T_outflow = T.where( (T >= 210) & (T <= 237) )\n",
    "    \n",
    "    Ni_insitu = Ni.where( (T < 210) )\n",
    "    T_insitu = T.where( (T < 210) )\n",
    "    \n",
    "    Ni_flight = Ni.sel( time=slice(time0, timef) )\n",
    "    T_flight = T.sel( time=slice(time0, timef) )\n",
    "\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni, T, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "    d['NiTh'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_outflow, T_outflow, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "    d['NiTh_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_insitu, T_insitu, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "    d['NiTh_insitu'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_flight )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_flight, T_flight, block_size=100, weights=wgts, bins=[Ni_bins,T_bins] )\n",
    "    d['NiTh_flight'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the Nih_* values in the Datasets above.\n",
    "for f, d, r in zip(fichiers, datasets, RHi_fi):\n",
    "    print(f)\n",
    "    fi = xr.open_dataset( basedir + f )\n",
    "    \n",
    "    # Convert kg-1 to L-1\n",
    "    rho = fi['rho']\n",
    "    Ni = fi['Ni'] * rho  / 1000.\n",
    "    Ni.name = fi['Ni'].name\n",
    "    T = fi['T']\n",
    "    RHI = fi['RHI']\n",
    "    time = fi['time']\n",
    "    \n",
    "    # Filter for non-negligible values\n",
    "    Ni = Ni.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "    RHI = RHI.where( (r > RHi_threshold ) & (Ni > Ni_threshold) )\n",
    "    #print('ALL')\n",
    "    #print(Ni.mean().values,RHI.mean().values)\n",
    "    #print(Ni.median().values,RHI.median().values)\n",
    "    \n",
    "    Ni_outflow = Ni.where( (T >= 210) & (T <= 237) )\n",
    "    RHI_outflow = RHI.where( (T >= 210) & (T <= 237) )\n",
    "    #print('OUTFLOW')\n",
    "    #print(Ni_outflow.mean().values,RHI_outflow.mean().values)\n",
    "    #print(Ni_outflow.median().values,RHI_outflow.median().values)\n",
    "    \n",
    "    Ni_insitu = Ni.where( (T < 210) )\n",
    "    RHI_insitu = RHI.where( (T < 210) )\n",
    "    #print('IN-SITU')\n",
    "    #print(Ni_insitu.mean().values,RHI_insitu.mean().values)\n",
    "    #print(Ni_insitu.median().values,RHI_insitu.median().values)\n",
    "    \n",
    "    Ni_flight = Ni.sel( time=slice(time0, timef) )\n",
    "    RHI_flight = RHI.sel( time=slice(time0, timef) )\n",
    "    #print('FLIGHT')\n",
    "    #print(Ni_flight.mean().values,RHI_flight.mean().values)\n",
    "    #print(Ni_flight.median().values,RHI_flight.median().values)\n",
    "    #print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "    \n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, qi )\n",
    "    # Weight by the total number of non-nan values\n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni, RHI, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "    d['NiRHih'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_outflow )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_outflow, RHI_outflow, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "    d['NiRHih_outflow'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_insitu )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_insitu, RHI_insitu, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "    d['NiRHih_insitu'] = h\n",
    "    \n",
    "    wgts = 1*xr.apply_ufunc( np.isfinite, Ni_flight )\n",
    "    wgts = wgts / wgts.sum( dim=['time','id'] ) * 100.\n",
    "    h = xhist( Ni_flight, RHI_flight, block_size=100, weights=wgts, bins=[Ni_bins,RHi_bins] )\n",
    "    d['NiRHih_flight'] = h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scratchdir = '/scratch/b/b380873/traj_pp/'\n",
    "names = [ 'qiNi-TRHi-2Dh-CLAMS-Tf_0V1M0A0R.nc', 'qiNi-TRHi-2Dh-ICON_0V1M0A0R.nc', \n",
    "          'qiNi-TRHi-2Dh-CLAMS-Tf_0V2M0A0R.nc', 'qiNi-TRHi-2Dh-ICON_0V2M0A0R.nc',\n",
    "          'qiNi-TRHi-2Dh-CLAMS_0V2M0A0R.nc']\n",
    "for n, d in zip(names, datasets):\n",
    "    d.to_netcdf( scratchdir + n )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nawdex-hackathon",
   "language": "python",
   "name": "nawdex-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
